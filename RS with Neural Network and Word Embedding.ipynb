{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System using Neural Netwrok with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adapted from https://www.kaggle.com/willkoehrsen/neural-network-embedding-recommendation-system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Json data for Books and articles\n",
    "data = []\n",
    "with open(\"DataSet/found_books_filtered.ndjson\", 'r') as f:\n",
    "    data = [json.loads(l) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "# filter books \n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books data for embedding\n",
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count no of occurances of each link to remove the most frequntly occuring wikilinks\n",
    "from collections import Counter, OrderedDict\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find set of wikilinks for each book and convert to a flattened list\n",
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count no of occurances of each link\n",
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "wikilink_counts = count_items(wikilinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most frequnt links\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove most frequntly occuring links\n",
    "to_remove = ['paperback','hardcover', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels', 'hardback', 'e-book']\n",
    "for t in to_remove:\n",
    "    wikilinks.remove(t)\n",
    "    wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to greater than 3 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index links\n",
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairs of Book index and link index to prepare train dataset\n",
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for book in books:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator for train data\n",
    "\n",
    "def generate_TrainData(pairs, n_positive = 50, negative_ratio = 1.0):\n",
    "    \n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "\n",
    "    neg_label = 0\n",
    "  \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        \n",
    "        # choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples \n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([ 6895., 25757., 29814., 22162., 28410.,  7206.]),\n",
       "  'link': array([  260., 22920., 11452.,  5588., 33217., 34924.])},\n",
       " array([1., 0., 0., 1., 0., 0.]))"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data sample\n",
    "next(generate_TrainData(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # add extra layer and loss function is binary cross entropy\n",
    "    \n",
    "    merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "    model = Model(inputs = [book, link], outputs = merged)\n",
    "    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 50)        1851000     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        2087900     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           book_embedding[0][0]             \n",
      "                                                                 link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1)            0           dot_product[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,938,902\n",
      "Trainable params: 3,938,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and show parameters\n",
    "model = book_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Declare generator for train data\n",
    "n_positive = 2000\n",
    "gen = generate_TrainData(pairs, n_positive, negative_ratio = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      " - 28s - loss: 0.6528 - accuracy: 0.6438\n",
      "Epoch 2/12\n",
      " - 29s - loss: 0.5396 - accuracy: 0.8087\n",
      "Epoch 3/12\n",
      " - 28s - loss: 0.4094 - accuracy: 0.8593\n",
      "Epoch 4/12\n",
      " - 30s - loss: 0.3548 - accuracy: 0.8779\n",
      "Epoch 5/12\n",
      " - 30s - loss: 0.3190 - accuracy: 0.8991\n",
      "Epoch 6/12\n",
      " - 31s - loss: 0.2968 - accuracy: 0.9105\n",
      "Epoch 7/12\n",
      " - 29s - loss: 0.2726 - accuracy: 0.9222\n",
      "Epoch 8/12\n",
      " - 29s - loss: 0.2542 - accuracy: 0.9299\n",
      "Epoch 9/12\n",
      " - 29s - loss: 0.2364 - accuracy: 0.9380\n",
      "Epoch 10/12\n",
      " - 31s - loss: 0.2476 - accuracy: 0.9293\n",
      "Epoch 11/12\n",
      " - 29s - loss: 0.2177 - accuracy: 0.9443\n",
      "Epoch 12/12\n",
      " - 27s - loss: 0.1970 - accuracy: 0.9537\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit_generator(gen, epochs = 12, steps_per_epoch = len(pairs) // n_positive, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model.save('RS_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "book_layer = model.get_layer('book_embedding')\n",
    "book_weights = book_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Book Embedding weights\n",
    "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Similar Books using Book Embedding weights\n",
    "def recommend_books(name, weights, n = 10):\n",
    "\n",
    "    index = book_index\n",
    "    reverseIndex = index_book\n",
    "\n",
    " \n",
    "    # Calculate dot product between book and all others\n",
    "    dists = np.dot(weights, weights[index[name]])\n",
    "\n",
    "    # Sort distance indexes from smallest to largest\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    recommended = sorted_dists[-n:]\n",
    "    \n",
    "    \n",
    "    max_width = max([len(reverseIndex[r]) for r in recommended])\n",
    "    \n",
    "    for r in reversed(recommended):\n",
    "        print(f'{reverseIndex[r]:{max_width}} Similarity:{str(round(dists[r],2))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War and Peace             Similarity:1.0\n",
      "Anna Karenina             Similarity:0.84\n",
      "The Brothers Karamazov    Similarity:0.82\n",
      "Crime and Punishment      Similarity:0.81\n",
      "Demons (Dostoevsky novel) Similarity:0.8\n",
      "The Idiot                 Similarity:0.77\n",
      "The Master and Margarita  Similarity:0.77\n",
      "Lord of the World         Similarity:0.76\n",
      "Poor Folk                 Similarity:0.75\n",
      "Cousin Bette              Similarity:0.74\n"
     ]
    }
   ],
   "source": [
    "recommend_books('War and Peace', book_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
